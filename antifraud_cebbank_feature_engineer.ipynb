{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读入函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分组统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroupStatCalc(data,list_hightype,list_midtype,list_lowtype,list_sensitype,list_risktype,list_accounttype,list_paytype,list_transtype,list_fail):\n",
    "    '''\n",
    "    input:\n",
    "    data: dataframe, 原始数据\n",
    "    list_hightype: list, 高频交易代码列表\n",
    "    list_midtype: list, 中频交易代码列表\n",
    "    list_lowtype: list, 低频交易代码列表\n",
    "    list_sensitype: list, 敏感交易代码列表\n",
    "    list_risktype: list, 风险交易代码列表\n",
    "    list_accounttype: list, 账户操作交易代码列表\n",
    "    list_paytype: list, 支付交易代码列表\n",
    "    list_transtype: list, 转账交易代码列表\n",
    "    list_fail: list, 失败交易代码列表\n",
    "    return:\n",
    "    result: dataframe, 变量大宽表, 其index为User_Id\n",
    "    '''\n",
    "    list_hightype = [xx.lower() for xx in list_hightype]\n",
    "    list_midtype = [xx.lower() for xx in list_midtype]\n",
    "    list_lowtype = [xx.lower() for xx in list_lowtype]\n",
    "    list_sensitype = [xx.lower() for xx in list_sensitype]\n",
    "    list_risktype = [xx.lower() for xx in list_risktype]\n",
    "    list_accounttype = [xx.lower() for xx in list_accounttype]\n",
    "    list_paytype = [xx.lower() for xx in list_paytype]\n",
    "    list_transtype = [xx.lower() for xx in list_transtype]\n",
    "    list_fail = [xx.lower() for xx in list_fail]\n",
    "    data['Txn_Tm'] = pd.to_datetime(data['Txn_Tm'],errors='coerce')\n",
    "\n",
    "    result = []\n",
    "    #总交易笔数\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['User_Id'].count()\n",
    "    tmp.name = 'TransNum'\n",
    "    result.append(tmp.copy()) \n",
    "    print('总交易笔数')\n",
    "    \n",
    "    #周末、周中交易数量以及交易数量标准差\n",
    "    weekday_map = {0:'Weekday',1:'Weekday',2:'Weekday',3:'Weekday',4:'Weekday',5:'Weekend',6:'Weekend'}\n",
    "    weekday_names = ['Weekday','Weekend']\n",
    "    data['tmp1_'] = data['Txn_Tm'].dt.weekday\n",
    "    data['tmp1_'] = data['tmp1_'].map(lambda xx:weekday_map.get(xx,np.nan))\n",
    "    data['tmp2_'] = data['Txn_Tm']\n",
    "    data = data.set_index('tmp2_')\n",
    "    data = data.to_period('W-SUN')\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    tmp = data.groupby(by=['User_Id','tmp1_'],sort=False)['tmp1_'].count()\n",
    "    tmp = tmp.unstack('tmp1_')\n",
    "    tmp.columns = tmp.columns+'Num'\n",
    "    tmp = tmp.reindex(columns=[col+'Num' for col in weekday_names])\n",
    "    tmp = tmp.fillna(0)\n",
    "    result.append(tmp.copy())    \n",
    "    \n",
    "    tmp = data.groupby(by=['User_Id','tmp1_','tmp2_'],sort=False)['tmp1_'].count()\n",
    "    tmp = tmp.groupby(level=['User_Id','tmp1_'],sort=False).std()\n",
    "    tmp = tmp.unstack('tmp1_')\n",
    "    tmp.columns = tmp.columns+'Std'\n",
    "    tmp = tmp.reindex(columns=[col+'Std' for col in weekday_names])\n",
    "    result.append(tmp.copy())\n",
    "    \n",
    "    data = data.drop(['tmp1_','tmp2_'],axis=1,errors='ignore')\n",
    "    print('周末、周中交易数量以及交易数量标准差')\n",
    "    \n",
    "    #一天四个时段交易数量以及交易数量标准差+午夜账户操作笔数+午夜支付笔数+午夜敏感交易笔数+午夜风险交易笔数\n",
    "    day_names=['Midnight','Morning','Afternoon','Evening']\n",
    "    \n",
    "    data['tmp2_'] = data['Txn_Tm']\n",
    "    data = data.set_index('tmp2_')\n",
    "    data = data.to_period('D')\n",
    "    data = data.reset_index()\n",
    "    \n",
    "    data['tmp3_'] = data['Txn_Tm'].dt.hour\n",
    "    data['tmp1_'] = np.nan\n",
    "    data.loc[(data['tmp3_']>=0)&(data['tmp3_']<=7), 'tmp1_'] = 'Midnight'\n",
    "    data.loc[(data['tmp3_']>=8)&(data['tmp3_']<=11), 'tmp1_'] = 'Morning'\n",
    "    data.loc[(data['tmp3_']>=12)&(data['tmp3_']<=17), 'tmp1_'] = 'Afternoon'\n",
    "    data.loc[(data['tmp3_']>=18)&(data['tmp3_']<=23), 'tmp1_'] = 'Evening'\n",
    "    \n",
    "    tmp = data.groupby(by=['User_Id','tmp1_'],sort=False)['tmp1_'].count()\n",
    "    tmp = tmp.unstack('tmp1_')\n",
    "    tmp.columns = tmp.columns+'Num'\n",
    "    tmp = tmp.reindex(columns=[col+'Num' for col in day_names])\n",
    "    tmp = tmp.fillna(0)\n",
    "    result.append(tmp.copy())\n",
    "    \n",
    "    tmp = data.groupby(by=['User_Id','tmp1_','tmp2_'],sort=False)['tmp1_'].count()\n",
    "    tmp = tmp.groupby(level=['User_Id','tmp1_'],sort=False).std()\n",
    "    tmp = tmp.unstack('tmp1_')\n",
    "    tmp.columns = tmp.columns+'Std'\n",
    "    tmp = tmp.reindex(columns=[col+'Std' for col in day_names])\n",
    "    result.append(tmp.copy())    \n",
    "    \n",
    "    data['tmp2_'] = data['Txn_Cd'].isin(list_accounttype).astype(int)\n",
    "    tmp = data.loc[data['tmp1_']=='Midnight'].groupby(by='User_Id',sort=False)['tmp2_'].sum()\n",
    "    tmp.name = 'MidnightAccountNum'\n",
    "    result.append(tmp.copy())\n",
    "    \n",
    "    data['tmp2_'] = data['Txn_Cd'].isin(list_paytype).astype(int)\n",
    "    tmp = data.loc[data['tmp1_']=='Midnight'].groupby(by='User_Id',sort=False)['tmp2_'].sum()\n",
    "    tmp.name = 'MidnightPayNum'\n",
    "    result.append(tmp.copy())    \n",
    "    \n",
    "    data['tmp2_'] = data['Txn_Cd'].isin(list_sensitype).astype(int)\n",
    "    tmp = data.loc[data['tmp1_']=='Midnight'].groupby(by='User_Id',sort=False)['tmp2_'].sum()\n",
    "    tmp.name = 'MidnightSensiNum'\n",
    "    result.append(tmp.copy())\n",
    "    \n",
    "    data['tmp2_'] = data['Txn_Cd'].isin(list_risktype).astype(int)\n",
    "    tmp = data.loc[data['tmp1_']=='Midnight'].groupby(by='User_Id',sort=False)['tmp2_'].sum()\n",
    "    tmp.name = 'MidnightRiskNum'\n",
    "    result.append(tmp.copy())\n",
    "    \n",
    "    data = data.drop(['tmp1_','tmp2_','tmp3_'],axis=1,errors='ignore')\n",
    "    print('一天四个时段交易数量以及交易数量标准差+午夜账户操作笔数+午夜支付笔数+午夜敏感交易笔数+午夜风险交易笔数')\n",
    "    \n",
    "    #高中低频交易数量\n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_hightype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'HighTypeNum'\n",
    "    result.append(tmp.copy())    \n",
    "    \n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_midtype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'MidTypeNum'\n",
    "    result.append(tmp.copy()) \n",
    "\n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_lowtype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'LowTypeNum'\n",
    "    result.append(tmp.copy())     \n",
    "    \n",
    "    print('高中低频交易数量')\n",
    "    \n",
    "    #敏感交易数量\n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_sensitype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'SensiTypeNum'\n",
    "    result.append(tmp.copy()) \n",
    "    \n",
    "    #风险交易数量\n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_risktype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'RiskTypeNum'\n",
    "    result.append(tmp.copy()) \n",
    "    \n",
    "    #账户操作交易数量\n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_accounttype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'AccountTypeNum'\n",
    "    result.append(tmp.copy()) \n",
    "    \n",
    "    #支付交易数量\n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_paytype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'PayTypeNum'\n",
    "    result.append(tmp.copy()) \n",
    "    \n",
    "    #转账交易数量\n",
    "    data['tmp_'] = data['Txn_Cd'].isin(list_transtype).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'TransTypeNum'\n",
    "    result.append(tmp.copy()) \n",
    "    \n",
    "    #失败交易数量\n",
    "    data['tmp_'] = data['Txn_Stat_Cd'].isin(list_fail).astype(int)\n",
    "    tmp = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    tmp.name = 'FailNum'\n",
    "    result.append(tmp.copy()) \n",
    "    \n",
    "    print('敏感、风险、账户操作、支付、转账、失败交易数量')\n",
    "\n",
    "    #IP地址数量\n",
    "    tmp = data.drop_duplicates(subset=['User_Id','Source_IP']).groupby(by='User_Id',sort=False)['Source_IP'].count()\n",
    "    tmp.name = 'IPNum'\n",
    "    result.append(tmp.copy())\n",
    "    print('IP地址数量')\n",
    "    \n",
    "    #合并数据\n",
    "    result = pd.concat(result,axis=1,join='outer')\n",
    "\n",
    "    #午夜特殊交易类型从数量转化为比率\n",
    "    result['MidnightAccountNum'] = result['MidnightAccountNum']/result['MidnightNum']\n",
    "    result['MidnightPayNum'] = result['MidnightPayNum']/result['MidnightNum']\n",
    "    result['MidnightSensiNum'] = result['MidnightSensiNum']/result['MidnightNum']\n",
    "    result['MidnightRiskNum'] = result['MidnightRiskNum']/result['MidnightNum']   \n",
    "    \n",
    "    result=result.rename(columns={'MidnightAccountNum','MidnightAccountPercent','MidnightPayNum','MidnightPayPercent',\n",
    "                                  'MidnightSensiNum','MidnightSensiPercent','MidnightRiskNum','MidnightRiskPercent'})\n",
    "\n",
    "    #将其他数量转化为比率    \n",
    "    col_num2percent=['Weekend','Weekday','Midnight','Morning','Afternoon','Evening',\n",
    "                     'HighType','MidType','LowType','SensiType','RiskType','AccountType',\n",
    "                     'PayType','Fail']\n",
    "    for col in col_num2percent:\n",
    "        result[col+'Num'] = result[col+'Num']/result['TransNum']\n",
    "    result = result.rename(columns={col+'Num':col+'Percent' for col in col_num2percent})\n",
    "    \n",
    "    result.index.name='User_Id'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前后变动次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IpFreqCalc(data):\n",
    "    '''\n",
    "    input:\n",
    "    data: dataframe, 原始数据\n",
    "    return:\n",
    "    result: dataframe, 包含User_Id和IpFreq\n",
    "    '''\n",
    "    #根据User_Id和Txn_Tm对数据排序\n",
    "    data['Txn_Tm'] = pd.to_datetime(data['Txn_Tm'],errors='coerce')\n",
    "    data = data.sort_values(['User_Id','Txn_Tm'],ascending=True)\n",
    "    #比较每条记录与前一条记录的IP地址\n",
    "    data['tmp_'] = data['Source_IP'].shift(1)\n",
    "    data['tmp_'] = (data['Source_IP']!=data['tmp_']).astype(int)\n",
    "    #用tag_标记每组User_Id的第一条记录\n",
    "    data['tag_'] = data['User_Id'].shift(1)\n",
    "    data['tag_'] = (data['User_Id']!=data['tag_'])\n",
    "    data.loc[data['tag_'],'tmp_'] = 0\n",
    "    #计算每组User_Id内IP地址变动次数\n",
    "    result = data.groupby(by='User_Id',sort=False)['tmp_'].sum()\n",
    "    result.name = 'IpFreq'\n",
    "    result.index.name='User_Id'\n",
    "    print('IP地址变动次数完成')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 连续最多次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConFailNumCalc(data,list_fail):\n",
    "    '''\n",
    "    input:\n",
    "    data: dataframe, 原始数据\n",
    "    list_fail: list, 失败交易代码列表\n",
    "    return:\n",
    "    result: dataframe, 包含User_Id和ConFailNum\n",
    "    '''\n",
    "    #失败交易代码大写转小写\n",
    "    list_fail = [xx.lower() for xx in list_fail]\n",
    "    #根据User_Id和Txn_Tm对数据排序\n",
    "    data['Txn_Tm'] = pd.to_datetime(data['Txn_Tm'],errors='coerce')\n",
    "    data = data.sort_values(['User_Id','Txn_Tm'],ascending=True)\n",
    "    #计算每组User_Id内连续失败交易笔数\n",
    "    data['tmp1_'] = (data['Txn_Stat_Cd'].isin(list_fail)==False).astype(int)\n",
    "    data['tmp2_'] = data['tmp1_'].cumsum()\n",
    "    data = data.set_index('User_Id')\n",
    "    data_last = data.groupby(level=['User_Id'],sort=False)['tmp2_'].last()\n",
    "    data_last = data_last.shift(1).fillna(0)\n",
    "    data['tmp2_'] = data['tmp2_']-data_last\n",
    "    data = data.reset_index()\n",
    "    #计算每组User_Id内连续失败最多交易笔数\n",
    "    result = data.groupby(by=['User_Id','tmp2_'],sort=False)['tmp2_'].count()-1\n",
    "    result = result.groupby(level=['User_Id'],sort=False).max()\n",
    "    result.name = 'ConFailNum'\n",
    "    result.index.name='User_Id'\n",
    "    print('连续失败最多交易笔数完成')\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 是否存在子列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RiskPatCountCalcGroupby(xx, risk_pat):\n",
    "    '''\n",
    "    辅助函数，对于每组User_Id的数据，判断是否存在试探性交易规律\n",
    "    input:\n",
    "    xx: 二元组(dataframe.groupby对象中的一个元素), 第一项为User_Id, 第二项为dataframe交易数据\n",
    "    risk_pat: list, 指定试探性交易规律\n",
    "    return:\n",
    "    tmp_result: dataframe, 包含User_Id和RiskPatCount\n",
    "    '''\n",
    "    df = xx[1].copy()\n",
    "    tmp = sum(list(df['tmp_'])[i:i+len(risk_pat)] == risk_pat for i in range(len(df)))\n",
    "    tmp_result = pd.DataFrame()\n",
    "    tmp_result['RiskPatCount'] = [tmp]\n",
    "    tmp_result['User_Id'] = xx[0]\n",
    "    return tmp_result\n",
    "\n",
    "def RiskPatCountCalc(data, list_type, list_fail, risk_pat):\n",
    "    '''\n",
    "    主体函数，判断是否存在试探性交易规律\n",
    "    input:\n",
    "    data: dataframe, 原始数据\n",
    "    list_type: list, 风险交易类型\n",
    "    list_fail: list, 失败交易代码\n",
    "    risk_pat: list, 指定试探性交易规律\n",
    "    return:\n",
    "    result: dataframe, 包含User_Id和RiskPatCount\n",
    "    '''\n",
    "    #交易类型和交易代码大写转小写\n",
    "    list_type = [xx.lower() for xx in list_type]\n",
    "    list_fail = [xx.lower() for xx in list_fail]\n",
    "    #根据User_Id和Txn_Tm对数据排序\n",
    "    data['Txn_Tm'] = pd.to_datetime(data['Txn_Tm'],errors='coerce')\n",
    "    data = data.sort_values(['User_Id','Txn_Tm'],ascending=True)\n",
    "    #判断每组User_Id内是否存在试探性交易\n",
    "    data['tmp_'] = (data['Txn_Stat_Cd'].isin(list_fail))&(data['Txn_Cd'].isin(list_type)).astype(int)\n",
    "    data_by = data.groupby(by='User_Id',sort=False)\n",
    "    data_list = list(map(lambda xx:RiskPatCountCalcGroupby(xx,risk_pat), data_by))\n",
    "    result = pd.concat(data_list)\n",
    "    print('是否存在试探性交易规律完成')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 滑动窗口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算每笔交易最近某段时间内的交易笔数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecentTransNumCalcGroupby(xx, col_time, col_time_before, col_out):\n",
    "    '''\n",
    "    辅助函数，对于每组User_Id的数据，计算每笔交易最近某段时间内（比如最近10分钟）的交易笔数\n",
    "    input:\n",
    "    xx: 二元组(dataframe, groupby对象中的一个元素), 第一项为User_Id, 第二项为dataframe交易数据\n",
    "    col_time: list, 交易时间对应的列名, 一般为Txn_Tm\n",
    "    col_time_before: list, 元素为交易时间减去某个时间段后的时间对应的列名, 如recent10m_time\n",
    "    col_out: list, 元素为计算得到的列名, 如Recent10mTransNum, 需要与col_time_before对应\n",
    "    return:\n",
    "    result: dataframe, 包含User_Id, Event_Id以及col_out中的字段\n",
    "    '''\n",
    "    df =xx[1].copy()\n",
    "    for i,col in enumerate(col_time_before):\n",
    "        df[col+'_start_index'] = df[col_time].values.searchsorted(df[col],side='right')\n",
    "        df[col+'_end_index'] = np.arange(df.shape[0])\n",
    "        result_tmp = df[col+'_end_index'] - df[col+'_start_index'] + 1\n",
    "        result_tmp.name = col_out[i]\n",
    "        result_tmp = pd.DataFrame(result_tmp)\n",
    "        result_tmp['User_Id'] = xx[0]\n",
    "        result_tmp['Event_Id'] = df['Event_Id']\n",
    "        if i==0:\n",
    "            result = result_tmp.copy()\n",
    "        else:\n",
    "            result = pd.merge(result,result_tmp,on=['Event_Id','User_Id'])       \n",
    "    return result\n",
    "            \n",
    "def RecentTransNumCalc(rawdata):\n",
    "    '''\n",
    "    主体函数，计算每笔交易最近某段时间内（比如最近10分钟）的交易笔数\n",
    "    input:\n",
    "    rawdata: dataframe, 原始数据\n",
    "    return:\n",
    "    result: dataframe, 包含User_Id, Event_Id以及统计字段\n",
    "    '''        \n",
    "    data = rawdata.copy()\n",
    "    #根据User_Id和Txn_Tm对数据排序\n",
    "    data['Txn_Tm'] = pd.to_datetime(data['Txn_Tm'],errors='coerce')\n",
    "    data = data.sort_values(by=['User_Id','Txn_Tm'])\n",
    "    #确定滑动窗口区间\n",
    "    data['recent10m_time'] = data['Txn_Tm'] - pd.DateOffset(minutes=10)\n",
    "    data['recent30m_time'] = data['Txn_Tm'] - pd.DateOffset(minutes=30)         \n",
    "    data['recent60m_time'] = data['Txn_Tm'] - pd.DateOffset(minutes=60)           \n",
    "    data['recent1_time'] = data['Txn_Tm'] - pd.DateOffset(days=1)            \n",
    "    data['recent3_time'] = data['Txn_Tm'] - pd.DateOffset(days=3)  \n",
    "    data['recent7_time'] = data['Txn_Tm'] - pd.DateOffset(days=7)  \n",
    "    data['recent30_time'] = data['Txn_Tm'] - pd.DateOffset(days=30)    \n",
    "    data_by = data.groupby(by='User_Id',sort=False)\n",
    "    #计算每组User_Id内每笔交易最近某段时间内的交易笔数\n",
    "    col_time_before_out_maps = {'recent10m_time':'Recent10mTransNum','recent30m_time':'Recent30mTransNum',\n",
    "                                'recent60m_time':'Recent60mTransNum','recent1_time':'Recent1TransNum',\n",
    "                                'recent3_time':'Recent3TransNum','recent7_time':'Recent7TransNum',\n",
    "                                'recent30_time':'Recent30TransNum'}\n",
    "    col_time = 'Txn_Tm'\n",
    "    col_time_before = {'recent10m_time','recent30m_time','recent60m_time','recent1_time',\n",
    "                       'recent3_time','recent7_time','recent30_time'}\n",
    "    col_out = [col_time_before_out_maps[col] for col in col_time_before]\n",
    "    data_list = list(map(lambda xx: RecentTransNumCalcGroupby(xx,col_time,col_time_before,col_out), data_by))\n",
    "    result = pd.concat(data_list)\n",
    "    print('每笔交易最近某段时间内的交易笔数完成')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算每笔交易最近某段时间内某种交易比例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecentXXXTransNumCalcGroupby(xx, col_time, col_time_before, col_num, col_out):\n",
    "    '''\n",
    "    辅助函数，对于每组User_Id的数据，计算每笔交易前某段时间内（比如最近十分钟）某种交易（比如敏感交易）比例\n",
    "    input:\n",
    "    xx: 二元组(dataframe, groupby对象中的一个元素), 第一项为User_Id, 第二项为dataframe交易数据\n",
    "    col_time: list, 交易时间对应的列名, 一般为Txn_Tm\n",
    "    col_time_before: list, 元素为交易时间减去某个时间段后的时间对应的列名, 如recent10m_time\n",
    "    col_num: list, 元素为是否符合某种交易对应的列名, 如sensi, 需要与col_time_before对应; \n",
    "             一般来源形如data['Txn_Cd'],isin(list_sensitype).astype(int)\n",
    "    col_out: list, 元素为计算得到的列名, 如Recent10mSensiTypeNum, 需要与col_time_before对应\n",
    "    return:\n",
    "    result: dataframe, 包含User_Id, Event_Id以及col_out中的字段\n",
    "    '''\n",
    "    df = xx[1].copy()\n",
    "    for i,col in enumerate(col_time_before):\n",
    "        df[col+'_start_index'] = df[col_time].values.searchsorted(df[col],side='right')\n",
    "        df.loc[df[col+'_start_index']>0,col+'_start_index'] = df.loc[df[col+'_start_index']>0,col+'_start_index']-1\n",
    "        df[col+'_end_index'] = np.arange(df.shape[0])\n",
    "        df[col+'_cumsum'] = df[col_num[i]].cumsum()\n",
    "        df[col+'_start_sum'] = df[col+'_cumsum'].iloc[df[col+'_start_index']].values\n",
    "        df[col+'_end_sum'] = df[col+'_cumsum'].iloc[df[col+'_end_index']].values\n",
    "        result_tmp = df[col+'_end_sum'] - df[col+'_start_sum']\n",
    "        result_tmp.name = col_out[i]\n",
    "        result_tmp = pd.DataFrame(result_tmp)\n",
    "        result_tmp['User_Id'] = xx[0]\n",
    "        result_tmp['Event_Id'] = df['Event_Id']\n",
    "        if i==0:\n",
    "            result = result_tmp.copy()\n",
    "        else:\n",
    "            result = pd.merge(result,result_tmp,on=['Event_Id','User_Id'])\n",
    "    return result\n",
    "\n",
    "def RecentXXXTransNumCalc(rawdata,list_sensitype,list_risktype,list_fail):\n",
    "    '''\n",
    "    主体函数, 计算每笔交易前某段时间内（比如最近十分钟）某种交易（比如敏感类型）比例\n",
    "    input:\n",
    "    rawdata: dataframe, 原始数据\n",
    "    list_sensitype: list, 敏感交易类型\n",
    "    list_risktype: list, 风险交易类型\n",
    "    list_fail: list, 失败交易代码列表\n",
    "    return:\n",
    "    result: dataframe, 包含User_Id, Event_Id以及统计字段\n",
    "    '''\n",
    "    data = rawdata.copy()\n",
    "\n",
    "    data['Txn_Tm'] = pd.to_datetime(data['Txn_Tm'],errors='coerce')\n",
    "    data = data.sort_values(by=['User_Id','Txn_Tm'])\n",
    "    #Recent60mSensiTypeNum\n",
    "    data['recent60m_time_sensi'] = data['Txn_Tm'] - pd.DateOffset(minutes=60)\n",
    "    data['recent60m_sensi_num'] = data['Txn_Cd'].isin(list_sensitype).astype(int)\n",
    "    #Recent60mRiskTypeNum\n",
    "    data['recent60m_time_risk'] = data['Txn_Tm'] - pd.DateOffset(minutes=60)\n",
    "    data['recent60m_risk_num'] = data['Txn_Cd'].isin(list_risktype).astype(int)\n",
    "    #Recent7FailNum\n",
    "    data['recent7_time_fail'] = data['Txn_Tm'] - pd.DateOffset(days=7)\n",
    "    data['recent7_fail_num'] = data['Txn_Cd'].isin(list_fail).astype(int)  \n",
    "    data_by = data.groupby(by='User_Id',sort=False)\n",
    "    \n",
    "    col_time_before_num_maps = {'recent60m_time_sensi':'recent60m_sensi_num',\n",
    "                                'recent60m_time_risk':'recent60m_risk_num',\n",
    "                                'recent7_time_fail':'recent7_fail_num'}\n",
    "    col_time_before_out_maps = {'recent60m_time_sensi':'Recent60mSensiTypeNum',\n",
    "                                'recent60m_time_risk':'Recent60mRiskTypeNum',\n",
    "                                'recent7_time_fail':'Recent7FailNum'} \n",
    "    col_time = 'Txn_Tm'\n",
    "    col_time_before = ['recent60m_time_sensi','recent60m_time_risk','recent7_time_fail']\n",
    "    col_num = [col_time_before_num_maps[col] for col in col_time_before]\n",
    "    col_out = [col_time_before_out_maps[col] for col in col_time_before]\n",
    "    data_list = list(map(lambda xx: RecentXXXTransNumCalcGroupby(xx,col_time,col_time_before,col_num,col_out),data_by))\n",
    "    result = pd.concat(data_list)\n",
    "    \n",
    "    print('每笔交易最近某段时间内某种交易比例')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算每笔交易与前X笔交易时间间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecentXTimeGapCalc(data, X=1):\n",
    "    '''\n",
    "    计算每笔交易与前X笔交易时间间隔\n",
    "    input:\n",
    "    data: dataframe, 原始数据\n",
    "    X: int, 前X笔\n",
    "    return:\n",
    "    result: dataframe, 包含Event_Id, User_Id, RecentXTimeGap, 其中会根据X的不同替换对应的列名\n",
    "    '''\n",
    "    data = data.fillna('')\n",
    "    data['Txn_Tm'] = pd.to_datetime(data['Txn_Tm'],errors='coerce')\n",
    "    data = data.sort_values(['User_Id','Txn_Tm'],ascending=True)\n",
    "    data['tmp_'] = data['Txn_Tm'].shift(X)\n",
    "    data['tmp_'] = (data['Txn_Tm']-data['tmp_']).astype('timedelta64[s]')/60\n",
    "    \n",
    "    #用tag_标记每组的前X条记录\n",
    "    data['tag_'] = data['User_Id'].shift(X)\n",
    "    data.loc[data['tag_']!=data['User_Id'],'tmp_'] = np.nan\n",
    "    \n",
    "    result = data[['Event_Id','User_Id','tmp_']].copy()\n",
    "    data = data.drop(['tmp_','tag_'],axis=1,errors='ignore')\n",
    "    result = result.rename(columns={'tmp_':'Recent%dTimeGap' %X})\n",
    "    \n",
    "    print('每笔交易与前X笔交易时间间隔')    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 滑动窗口内的统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecentStatCalc(data):\n",
    "    '''\n",
    "    滑动窗口内的统计\n",
    "    input:\n",
    "    data: dataframe, 滑动窗口中间表, 应该有User_Id, Event_Id和其他数据列\n",
    "    return:\n",
    "    result: dataframe, 滑动窗口相关变量的大宽表, 其index为User_Id\n",
    "    '''\n",
    "    result = []       \n",
    "    \n",
    "    #每笔交易前60分钟内交易笔数中位数、标准差\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent60mTransNum'].median()\n",
    "    tmp.name = 'Recent60mTransNumMed'\n",
    "    result.append(tmp.copy())\n",
    "\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent60mTransNum'].std()\n",
    "    tmp.name = 'Recent60mTransNumStd'\n",
    "    result.append(tmp.copy())        \n",
    "        \n",
    "    #每笔交易前30天内交易笔数中位数、标准差\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent30TransNum'].median()\n",
    "    tmp.name = 'Recent30TransNumMed'\n",
    "    result.append(tmp.copy())\n",
    "\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent30TransNum'].std()\n",
    "    tmp.name = 'Recent30TransNumStd'\n",
    "    result.append(tmp.copy())         \n",
    "            \n",
    "    #每笔交易与前3、10笔交易时间间隔中位数、标准差\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent3TimeGap'].median()\n",
    "    tmp.name = 'Recent3TimeGapMed'\n",
    "    result.append(tmp.copy())\n",
    "\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent3TimeGap'].std()\n",
    "    tmp.name = 'Recent3TimeGapStd'\n",
    "    result.append(tmp.copy())            \n",
    "            \n",
    "    #每笔交易前60分钟内敏感交易类型比例中位数\n",
    "    data['Recent60mSensiTypePercent'] = data['Recent60mSensiTypeNum']/data['Recent60mTransNum']   \n",
    "    data['Recent7FailPercent'] = data['Recent7FailNum']/data['Recent7TransNum']\n",
    "\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent60mSensiTypePercent'].median()\n",
    "    tmp.name = 'Recent60mSensiTypePercentMed'\n",
    "    result.append(tmp.copy()) \n",
    "\n",
    "    #每笔交易前7天内失败交易比率中位数\n",
    "    tmp = data.groupby('User_Id',sort=False)['Recent7FailPercent'].median()\n",
    "    tmp.name = 'Recent7FailPercentMed'\n",
    "    result.append(tmp.copy())    \n",
    "\n",
    "    result = pd.concat(result,axis=1)\n",
    "    result.index.name = 'User_Id'\n",
    "    \n",
    "    print('滑动窗口统计量')  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算大宽表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################main##################################\n",
    "path = '/usr/local/workspace/'\n",
    "os.chdir(path)\n",
    "\n",
    "#读入各类交易类型划分标准\n",
    "maps = pd.read_excel('resource/电子银行交易类型汇总.xlsx',sheetname=None)\n",
    "\n",
    "list_hightype = list(maps['高频交易']['Txn_Cd'])\n",
    "list_midtype = list(maps['中频交易']['Txn_Cd'])\n",
    "list_lowtype = list(maps['低频交易']['Txn_Cd'])\n",
    "list_sensitype = list(maps['敏感交易']['Txn_Cd'])\n",
    "list_risktype = list(maps['风险交易']['Txn_Cd'])\n",
    "list_accounttype = list(maps['账户操作交易']['Txn_Cd'])\n",
    "list_paytype = list(maps['支付交易']['Txn_Cd'])\n",
    "list_transtype = list(maps['转账交易']['Txn_Cd'])\n",
    "list_querytype = list(maps['查询交易']['Txn_Cd'])\n",
    "list_fail = list(maps['失败交易']['Txn_Stat_Cd'])\n",
    "\n",
    "for i in [0,1,2,3,4]:\n",
    "    #读取原始数据\n",
    "    data_select = pd.read_csv('/data1/sample_bigtable/data_select_%d.csv'%i,dtype=str)\n",
    "    data = data_select.drop(['Txn_Amt','Currency_Cd','Channel_Type','Login_Type_Cd','Cust_Acct_Type_Cd','Event_Type_Cd'],axis=1,errors='ignore')\n",
    "    data = data.fillna('')\n",
    "    data = data.applymap(lambda xx:xx.strip())\n",
    "    print('原始数据读取成功')\n",
    "    \n",
    "    #计算分组统计量\n",
    "    result = GroupStatCalc(data=data,list_hightype=list_hightype,list_midtype=list_midtype,list_lowtype=list_lowtype,\n",
    "                             list_sensitype=list_sensitype,list_risktype=list_risktype,list_accounttype=list_accounttype,\n",
    "                             list_paytype=list_paytype,list_transtype=list_transtype,list_querytype=list_querytype,list_fail=list_fail)\n",
    "    result.to_csv('/data1/sample_bigtable/bigtable_groupstat_%d.csv'%i,index=True,header=True,encoding='utf-8')\n",
    "    print('分组统计变量计算成功')\n",
    "    \n",
    "    #计算前后变动次数\n",
    "    result = IpFreqCalc(data)\n",
    "    result.to_csv('/data1/sample_bigtable/bigtable_ipfreq_%d.csv'%i,index=True,header=True,encoding='utf-8')\n",
    "    print('IpFreq计算成功')\n",
    "    \n",
    "    #计算连续最多次数\n",
    "    result = ConFailNumCalc(data,list_fail=list_fail)\n",
    "    result.to_csv('/data1/sample_bigtable/bigtable_confailnum_%d.csv'%i,index=True,header=True,encoding='utf-8')\n",
    "    print('ConFailNum计算成功')\n",
    "    \n",
    "    #计算是否存在子列\n",
    "    risk_pat = [True,False,True,False,True,False]\n",
    "    result = RiskPatCountCalc(data=data,list_type=list_risktype,list_fail=list_fail,risk_pat=risk_pat)\n",
    "    result_risk.columns = ['RiskPatCount','User_Id']\n",
    "    result_risk.index = range(len(result_risk))\n",
    "    result.to_csv('/data1/sample_bigtable/bigtable_riskpatcount_%d.csv'%i,index=True,header=True,encoding='utf-8')\n",
    "    print('RiskPatCount计算成功')\n",
    "    \n",
    "    #计算滑动窗口\n",
    "    result = RecentTransNumCalc(data)\n",
    "    result_tmp = RecentXXXTransNumCalc(data,list_sensitype,list_risktype,list_fail)\n",
    "    result = pd.merge(result,result_tmp,on=['User_Id','Event_Id'],how='outer')\n",
    "    \n",
    "    tmp = RecentXTimeGapCalc(data,X=3)\n",
    "    del tmp['User_Id']\n",
    "    result = pd.merge(result,tmp,on='Event_Id')\n",
    "\n",
    "    result.to_csv('/data1/sample_bigtable/bigtable_recentstatmiddle_%d.csv'%i,index=None,header=True,encoding='utf-8')\n",
    "    print('滑动窗口中间表计算成功')\n",
    "    \n",
    "    #计算滑动窗口统计量\n",
    "    result = RecentStatCalc(result)\n",
    "    result.to_csv('/data1/sample_bigtable/bigtable_recentstat_%d.csv'%i,index=None,header=True,encoding='utf-8')\n",
    "    print('滑动窗口统计量计算成功')\n",
    "    \n",
    "    #合并所有变量\n",
    "    result_groupstat = pd.read_csv('/data1/sample_bigtable/bigtable_groupstat_%d.csv'%i,converters={'User_Id':str})\n",
    "    result_ipfreq = pd.read_csv('/data1/sample_bigtable/bigtable_ipfreq_%d.csv'%i,converters={'User_Id':str})\n",
    "    result_confailnum = pd.read_csv('/data1/sample_bigtable/bigtable_confailnum_%d.csv'%i,converters={'User_Id':str})\n",
    "    result_riskpatcount = pd.read_csv('/data1/sample_bigtable/bigtable_riskpatcount_%d.csv'%i,converters={'User_Id':str})\n",
    "    result_recentstat = pd.read_csv('/data1/sample_bigtable/bigtable_recentstat_%d.csv'%i,converters={'User_Id':str})\n",
    "    \n",
    "    result = pd.merge(result_groupstat,result_ipfreq,on='User_Id')\n",
    "    result = pd.merge(result,result_confailnum,on='User_Id')\n",
    "    result = pd.merge(result,result_riskpatcount,on='User_Id')\n",
    "    result = pd.merge(result,result_recentstat,on='User_Id')\n",
    "    \n",
    "    #填补缺失\n",
    "    fill_values={}\n",
    "    for col in result.columns.tolist():\n",
    "        if col == 'User_Id':\n",
    "            continue\n",
    "        if col.endswith('Std'):\n",
    "            fill_values[col] = 0\n",
    "        else:\n",
    "            fill_values[col] = result[col].median()\n",
    "    result = result.fillna(fill_values)\n",
    "    \n",
    "    result.to_csv('/data1/sample_bigtable/bigtable_final_%d.csv'%i,index=None,header=True,encoding='utf-8')\n",
    "    print('第%d张大宽表计算成功'%i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
